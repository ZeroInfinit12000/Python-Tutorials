{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Z0      Z1      Z2      Z3     Z4      W0        W1        W2        W3     \\\n",
       " 0  1.0   3.782067  0.0  0.799931  1.0  0.300954  0.318778  0.950390 -0.465867   \n",
       " 1  1.0  32.989599  0.0  0.421178  1.0 -0.678573 -1.163724  0.677711  0.319513   \n",
       " 2  0.0  32.715236  0.0  0.348849  1.0  0.745811  0.920205  0.756560 -1.711820   \n",
       " 3  0.0  12.876439  1.0  0.808515  1.0  0.651129 -0.525222  0.028904 -0.749755   \n",
       " 4  1.0  29.343564  0.0  0.969060  1.0 -1.018517 -0.803864  1.558629 -0.225619   \n",
       " \n",
       "   W4  v0      y      \n",
       " 0  3   1  31.591271  \n",
       " 1  2   1  35.853303  \n",
       " 2  1   1  35.258663  \n",
       " 3  1   1  33.340179  \n",
       " 4  1   1  39.343054  ,\n",
       " count    1000.000000\n",
       " mean        0.991000\n",
       " std         0.094488\n",
       " min         0.000000\n",
       " 25%         1.000000\n",
       " 50%         1.000000\n",
       " 75%         1.000000\n",
       " max         1.000000\n",
       " Name: v0, dtype: float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dowhy.datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Syntethic Data for linear problems:\n",
    "\n",
    "# With this we generate a syntethic dataset, in order to see wheater our methods work well or not\n",
    "\n",
    "# The real model is:\n",
    "# y=b0+b1*v0+b2*w0+b3*w1+b4*w2+b5*w3+b6*w4+[b7@x1[True=1]+b8*x1[True=2]+b9*x1[True=3]+b10*x0 + the inetreaction of all the x's\n",
    "# with v0] + u\n",
    "\n",
    "# Basically, here we assume that we observe all factors that might be realated with the treatment and with the outcome\n",
    "# and that they affect (specificaty problem) linearly the outcome\n",
    "\n",
    "df = dowhy.datasets.linear_dataset(\n",
    "    beta=3,  # This is the real coefficient of the treament in the lineal model, effect of the treamtment on the outcome\n",
    "    # Number of variables that affect both the treatment (are in the error e) and the outcome (cov(v0,Wi) not 0).\n",
    "    # I guess is the ATE\n",
    "    num_common_causes=5,\n",
    "    # This are basically ommited variables, hence if you do y=b0+b1*v0+b2*w0+b3*w1+b4*w2+b5*w3+b6*w4+u via OLS, you got  that\n",
    "    # Now the b1 consistely estimate b1!!! (which is the beta).\n",
    "    # All W's are independent from one another\n",
    "    # Causality :w->v0,w->y.\n",
    "    # I beleive this are exogenous variables, i.e., once inside the regression, they are not related anymore with the error term...\n",
    "    num_discrete_common_causes=1,\n",
    "    # This says one of the W's is discrete\n",
    "    num_instruments=5,\n",
    "    # Number of correct instrumental variables for the treatment: Cov(v0, Zi)!=0, Cov(e, zi)=0. Hence, in the regression\n",
    "    # y=b0+b1*v0+e, this variables are related in covariance with v0, but are in no way related in covariance with e (where in the e we got the W's)\n",
    "    # Causality :z->v0->Y0. Notice that in standard regression the problem always is: error->v0->Y0<-error, hence now we find some\n",
    "    # Z that just affects v0 and is not realeted to error...\n",
    "    # num_frontdoor_variables=1,\n",
    "    # Basically, this are kind of like IV, but with the difference that now you got the FD0 as a mechanism though which\n",
    "    # v0 causes Y0. This are endogenous withing the model itself!!!\n",
    "    # Hence, now you estimate: v0->FD0->Y0.They have their own regression and stuff. Appers to be good for some autoselection problems, though perhaps less than IV. Check:The Book of Why\n",
    "    num_treatments=1,\n",
    "    # This is the number of  treatment variable we are intrested in estimate.\n",
    "    num_samples=1000,\n",
    "    # num_effect_modifiers=2,\n",
    "    # This are effect modifiers. Esentially, this is the case when the effect of the treatment depend on specific caracteristics\n",
    "    # of the individual. Say, if one is a Man or a Woman. This is why, to estimate the B1 correctly, you must include the X's and the\n",
    "    # interaction of the X's with the treatment v0 (not with W's, since they are exogenous once inside the equation and not realted\n",
    "    # witn X never)\n",
    "    # num_discrete_effect_modifiers=1,\n",
    "    # This will make the effect modifiers take on categorical values, like {0,1,2,3}, hence, in the regresion due to perfect\n",
    "    # multicolinearity problems, just include indicators variables for 3 of the cases. Patsy or Fomulaic will do this automatically for you\n",
    "    treatment_is_binary=True,\n",
    "    # Traeat binarity\n",
    "    outcome_is_binary=False,\n",
    "    # Outcome binarity\n",
    ")\n",
    "\n",
    "data = df[\"df\"]\n",
    "y = 33*data.v0+abs(5*data.W0**2+6*np.log(abs(data.W1))-0.9*data.W3**5/(data.W2+5)\n",
    "                   )**abs(data.W1)-(5*data.W4.astype(float)-data.W2**6+6*data.W1*data.W2)/5\n",
    "\n",
    "\n",
    "def flatten_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    df.loc[df[column] < lower_bound, column] = df[column].loc[(\n",
    "        df[column] >= lower_bound) & (df[column] <= upper_bound)].iloc[0]\n",
    "    df.loc[df[column] > upper_bound, column] = df[column].loc[(\n",
    "        df[column] >= lower_bound) & (df[column] <= upper_bound)].iloc[-1]\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "y = flatten_outliers(pd.DataFrame(y), 0)\n",
    "\n",
    "y.describe()\n",
    "Z0 = data.Z0\n",
    "Z1 = 33*data.Z1\n",
    "new_data = data\n",
    "new_data[\"y\"] = y\n",
    "new_data[\"Z0\"] = Z0\n",
    "new_data[\"Z1\"] = Z1\n",
    "df[\"df\"] = new_data\n",
    "data = df[\"df\"]\n",
    "data[\"v0\"] = data[\"v0\"].astype(\"int\")\n",
    "# Note: The real param Now is beta=33!!!\n",
    "data.head(), data.v0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.038701\n",
      "         Iterations 10\n"
     ]
    }
   ],
   "source": [
    "# Probit to take probs\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from formulaic import model_matrix\n",
    "treatment, controls = model_matrix(\"v0~W0+W1+W2+W3+W4\", data)\n",
    "probit = sm.Probit(treatment, controls).fit()\n",
    "# predictions\n",
    "pscore = probit.predict()\n",
    "# add to previous data\n",
    "new_data = pd.concat([data, pd.DataFrame(pscore)], axis=1)\n",
    "new_data.columns.values[-1] = \"pscore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To gain access to an instance object simply pass the value from 'ID' to dtale.get_instance(ID)\n",
      "\n",
      "ID Name                    URL                   \n",
      "1       http://DESKTOP-UR5COVK:40000/dtale/main/1\n",
      "        http://DESKTOP-UR5COVK:40000/dtale/main/1\n",
      "2       http://DESKTOP-UR5COVK:40000/dtale/main/2\n",
      "        http://DESKTOP-UR5COVK:40000/dtale/main/2\n"
     ]
    }
   ],
   "source": [
    "import dtale\n",
    "from dataprep.eda import create_report\n",
    "# create_report(data).show_browser()\n",
    "# This works best on the web web. Is good, though i guess the 2 backwards is better\n",
    "\n",
    "d = dtale.show(new_data.dropna())\n",
    "\n",
    "tmp = d.data.copy()\n",
    "tmp['d'] = 4\n",
    "d.open_browser()\n",
    "d._data_id  # The process's data identifier.\n",
    "d._url  # The url to access the process.\n",
    "# Returns a new reference to the instance running at that data_id.\n",
    "d2 = dtale.get_instance(d._data_id)\n",
    "# Prints a list of all ids & urls of running D-Tale sessions.\n",
    "dtale.instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.033\n",
      "Model:                            OLS   Adj. R-squared:                  0.031\n",
      "Method:                 Least Squares   F-statistic:                     12.14\n",
      "Date:                Mon, 11 Mar 2024   Prob (F-statistic):           6.15e-06\n",
      "Time:                        07:11:08   Log-Likelihood:                -4114.1\n",
      "No. Observations:                1000   AIC:                             8234.\n",
      "Df Residuals:                     997   BIC:                             8249.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:                  hc3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     78.4866     30.968      2.534      0.011      17.790     139.183\n",
      "v0            27.9411      6.027      4.636      0.000      16.128      39.754\n",
      "pscore       -65.4632     31.170     -2.100      0.036    -126.556      -4.370\n",
      "==============================================================================\n",
      "Omnibus:                      465.965   Durbin-Watson:                   2.143\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1883.970\n",
      "Skew:                           2.283   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.936   Cond. No.                         109.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.081\n",
      "Model:                            OLS   Adj. R-squared:                  0.074\n",
      "Method:                 Least Squares   F-statistic:                     6.993\n",
      "Date:                Mon, 11 Mar 2024   Prob (F-statistic):           5.31e-09\n",
      "Time:                        07:11:09   Log-Likelihood:                -4088.3\n",
      "No. Observations:                1000   AIC:                             8195.\n",
      "Df Residuals:                     991   BIC:                             8239.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:                  hc3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     14.4587      6.739      2.146      0.032       1.251      27.667\n",
      "W4[T.1]       -0.7620      1.174     -0.649      0.516      -3.063       1.539\n",
      "W4[T.2]        0.8501      1.333      0.638      0.524      -1.762       3.462\n",
      "W4[T.3]       -0.7221      1.299     -0.556      0.578      -3.267       1.823\n",
      "v0            24.0469      6.723      3.577      0.000      10.870      37.224\n",
      "W0             0.5830      0.552      1.056      0.291      -0.499       1.665\n",
      "W1            -2.2248      0.638     -3.486      0.000      -3.476      -0.974\n",
      "W2             2.6916      0.633      4.254      0.000       1.451       3.932\n",
      "W3            -0.7024      0.472     -1.489      0.136      -1.627       0.222\n",
      "==============================================================================\n",
      "Omnibus:                      470.568   Durbin-Watson:                   2.109\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2015.902\n",
      "Skew:                           2.276   Prob(JB):                         0.00\n",
      "Kurtosis:                       8.259   Cond. No.                         31.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.102\n",
      "Model:                            OLS   Adj. R-squared:                  0.094\n",
      "Method:                 Least Squares   F-statistic:                     9.375\n",
      "Date:                Mon, 11 Mar 2024   Prob (F-statistic):           9.10e-14\n",
      "Time:                        07:11:09   Log-Likelihood:                -4077.0\n",
      "No. Observations:                1000   AIC:                             8174.\n",
      "Df Residuals:                     990   BIC:                             8223.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:                  hc3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    139.4930     40.957      3.406      0.001      59.219     219.767\n",
      "W4[T.1]       -2.7584      1.251     -2.204      0.028      -5.211      -0.306\n",
      "W4[T.2]        0.4892      1.331      0.367      0.713      -2.120       3.098\n",
      "W4[T.3]       -1.4400      1.280     -1.125      0.261      -3.949       1.069\n",
      "v0            27.7317      5.007      5.539      0.000      17.918      37.545\n",
      "pscore      -128.6486     41.312     -3.114      0.002    -209.618     -47.679\n",
      "W0             1.5314      0.601      2.549      0.011       0.354       2.709\n",
      "W1            -1.4583      0.691     -2.110      0.035      -2.813      -0.104\n",
      "W2             3.4914      0.670      5.209      0.000       2.178       4.805\n",
      "W3             0.0573      0.496      0.115      0.908      -0.916       1.030\n",
      "==============================================================================\n",
      "Omnibus:                      461.076   Durbin-Watson:                   2.110\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1970.203\n",
      "Skew:                           2.220   Prob(JB):                         0.00\n",
      "Kurtosis:                       8.251   Cond. No.                         187.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n"
     ]
    }
   ],
   "source": [
    "print(sm.OLS.from_formula(\"y~v0+pscore\",data).fit(cov_type=\"hc3\").summary())\n",
    "print(sm.OLS.from_formula(\"y~v0+W0+W1+W2+W3+W4\",data).fit(cov_type=\"hc3\").summary())\n",
    "print(sm.OLS.from_formula(\"y~v0+pscore+W0+W1+W2+W3+W4\",data).fit(cov_type=\"hc3\").summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
